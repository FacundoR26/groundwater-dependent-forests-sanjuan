{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1D0SnzZWGn4i1uBreobcQEjCqp9h72dU9",
      "authorship_tag": "ABX9TyMW37M30j4HmFGPvmhW/up1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FacundoR26/groundwater-dependent-forests-sanjuan/blob/main/Identificacion_bosques2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Identificacion de bosques asociados a la napa usando RF**"
      ],
      "metadata": {
        "id": "VVv9RJtSfk9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Obtencion de indices NDVI y NDWI***\n",
        "Obtendremos los indices para la estacion seca y humeda desde el año 2015 hasta el 2025 y los visualizaremas con la libreria geemap."
      ],
      "metadata": {
        "id": "KQVVAqT56VyZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaWuv3m53QZM"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import geemap\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion que calcula NDVI y NDWI"
      ],
      "metadata": {
        "id": "ccQ_17BtjCef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcion para calcular NDVI y NDWI\n",
        "def add_indices(image):\n",
        "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
        "    return image.addBands([ndvi, ndwi])"
      ],
      "metadata": {
        "id": "Jk5O9nfX6-PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definimos el roi"
      ],
      "metadata": {
        "id": "85ciazNxi-1y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55a8cbff"
      },
      "source": [
        "# Replace 'users/your_username/your_asset_path' with the actual asset ID of your shapefile in Earth Engine\n",
        "shapefile_asset_id = 'users/facu_ruarte/puntos_freatica_exp'\n",
        "\n",
        "# Load the shapefile asset from Earth Engine\n",
        "try:\n",
        "    roi_feature_collection = ee.FeatureCollection(shapefile_asset_id)\n",
        "    print(f\"Successfully loaded shapefile asset: {shapefile_asset_id}\")\n",
        "\n",
        "    # Extract the geometry of the first feature\n",
        "    # Assuming the shapefile asset contains at least one feature\n",
        "    roi_geometry = roi_feature_collection.first().geometry()\n",
        "    print(\"Extracted geometry from the first feature in the collection.\")\n",
        "\n",
        "\n",
        "    # Check if Map object exists, if not, create one\n",
        "    if 'Map' not in locals():\n",
        "        Map = geemap.Map()\n",
        "        print(\"geemap.Map() object created.\")\n",
        "\n",
        "    # Center the map on the loaded shapefile geometry\n",
        "    Map.centerObject(roi_geometry)\n",
        "    print(\"Centered map on the loaded shapefile geometry.\")\n",
        "\n",
        "    # You can optionally add the shapefile to the map as a layer for visualization\n",
        "    Map.addLayer(roi_feature_collection, {}, 'Region of Interest (from Asset)')\n",
        "    print(\"Added shapefile layer to the map.\")\n",
        "\n",
        "    # Display the map\n",
        "    display(Map)\n",
        "\n",
        "except ee.EEException as e:\n",
        "    print(f\"Error loading Earth Engine asset: {e}\")\n",
        "    print(f\"Please ensure the asset ID '{shapefile_asset_id}' is correct and the asset is publicly accessible or in your Earth Engine account.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion para obtener medianas por estacion\n",
        "La estacion seca estara compuesta por los meses Febrero, Marzo y Abril, y la estacion humeda por los meses Julio, Agosto y Septiembre, usaremos la mediana para obtener un valor representativo para cada estacion."
      ],
      "metadata": {
        "id": "rat4oaJ67VoF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00c735fd"
      },
      "source": [
        "# Funcion para obtener medianas por estacion\n",
        "def get_season_median(year, months, season_name):\n",
        "    start = ee.Date.fromYMD(year, months[0], 1)\n",
        "    end = ee.Date.fromYMD(year, months[-1], 30)\n",
        "\n",
        "    collection = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "                  .filterDate(start, end)\n",
        "                  .filterBounds(roi_geometry) # Use the extracted geometry\n",
        "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30)) # Further relaxed cloud filter\n",
        "                  .map(add_indices)\n",
        "                  .select(['NDVI', 'NDWI'])) # Select only the calculated index bands\n",
        "\n",
        "    median = collection.median().clip(roi_geometry) # Clip using the extracted geometry\n",
        "    return median.set({'year': year, 'season': season_name})\n",
        "\n",
        "# Generar imagenes por año y estacion desde el 2015 hasta el 2025\n",
        "dry_months = [2, 3, 4]\n",
        "wet_months = [7, 8, 9]\n",
        "\n",
        "dry_list = []\n",
        "wet_list = []\n",
        "\n",
        "for year in range(2019, 2026):\n",
        "    dry = get_season_median(year, dry_months, 'dry')\n",
        "    wet = get_season_median(year, wet_months, 'wet')\n",
        "    dry_list.append(dry)\n",
        "    wet_list.append(wet)\n",
        "\n",
        "dry_collection = ee.ImageCollection(dry_list)\n",
        "wet_collection = ee.ImageCollection(wet_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizamos el NDVI y NDWI de un año en especifico\n",
        "Se usa el stack de bandas obtenido en la linea anterior"
      ],
      "metadata": {
        "id": "5VJHvnPCjpTw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe882b64"
      },
      "source": [
        "# Visualizar NDVI y NDWI de año específico\n",
        "year_to_show = 2022 # Especificar el año a observar\n",
        "\n",
        "dry_collection_year = dry_collection.filter(ee.Filter.eq('year', year_to_show))\n",
        "wet_collection_year = wet_collection.filter(ee.Filter.eq('year', year_to_show))\n",
        "\n",
        "# Adjusted NDVI parameters to include negative values\n",
        "ndvi_params = {'min': -1, 'max': 1, 'palette': ['red','white', 'green']}\n",
        "ndwi_params = {'min': -1, 'max': 1, 'palette': ['brown','white', 'cyan']}\n",
        "\n",
        "# Add checks to ensure collections are not empty before getting the first image and adding layers\n",
        "if dry_collection_year.size().getInfo() > 0:\n",
        "    dry_img = dry_collection_year.first()\n",
        "    # Clip the image first using the extracted geometry, then select bands\n",
        "    dry_img_clipped = dry_img.clip(roi_geometry)\n",
        "    # Swapped \"Seco\" and \"Húmedo\" labels (interpreted)\n",
        "    Map.addLayer(dry_img_clipped.select('NDVI'), ndvi_params, f'NDVI Húmedo (interpreted) {year_to_show}')\n",
        "    Map.addLayer(dry_img_clipped.select('NDWI'), ndwi_params, f'NDWI Húmedo (interpreted) {year_to_show}')\n",
        "else:\n",
        "    print(f\"No data available with NDVI and NDWI bands for dry season {year_to_show} in the selected region.\")\n",
        "\n",
        "if wet_collection_year.size().getInfo() > 0:\n",
        "    wet_img = wet_collection_year.first()\n",
        "    # Clip the image first using the extracted geometry, then select bands\n",
        "    wet_img_clipped = wet_img.clip(roi_geometry)\n",
        "    # Swapped \"Seco\" and \"Húmedo\" labels (interpreted)\n",
        "    Map.addLayer(wet_img_clipped.select('NDVI'), ndwi_params, f'NDVI Seco (interpreted) {year_to_show}')\n",
        "    Map.addLayer(wet_img_clipped.select('NDWI'), ndwi_params, f'NDWI Seco (interpreted) {year_to_show}')\n",
        "else:\n",
        "    print(f\"No data available with NDVI and NDWI bands for wet season {year_to_show} in the selected region.\")\n",
        "\n",
        "# Observar valores minimos y maximos\n",
        "print(\"Layers on the map:\")\n",
        "for layer in Map.layers:\n",
        "    print(layer.name)\n",
        "\n",
        "# Optional: Inspect image data within the drawn region (may take some time)\n",
        "print(f\"\\nInspecting image data for {year_to_show} dry season:\")\n",
        "# Use roi_geometry for reducing region\n",
        "if dry_collection_year.size().getInfo() > 0:\n",
        "    dry_stats = dry_collection_year.first().select(['NDVI', 'NDWI']).reduceRegion(\n",
        "         reducer=ee.Reducer.minMax(),\n",
        "         geometry=roi_geometry,\n",
        "         scale=100, # Adjust scale as needed\n",
        "         maxPixels=1e9\n",
        "    )\n",
        "    print(dry_stats.getInfo())\n",
        "else:\n",
        "    print(\"Dry season image collection for the specified year is empty. Cannot compute statistics.\")\n",
        "\n",
        "\n",
        "print(f\"\\nInspecting image data for {year_to_show} wet season:\")\n",
        "# Use roi_geometry for reducing region\n",
        "if wet_collection_year.size().getInfo() > 0:\n",
        "    wet_stats = wet_collection_year.first().select(['NDVI', 'NDWI']).reduceRegion(\n",
        "         reducer=ee.Reducer.minMax(),\n",
        "         geometry=roi_geometry,\n",
        "         scale=100, # Adjust scale as needed\n",
        "         maxPixels=1e9\n",
        "    )\n",
        "    print(wet_stats.getInfo())\n",
        "else:\n",
        "     print(\"Wet season image collection for the specified year is empty. Cannot compute statistics.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the map\n",
        "display(Map)"
      ],
      "metadata": {
        "id": "BXFCcj7LmNL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Unificación*** y Preparación de los Datos para el Modelo"
      ],
      "metadata": {
        "id": "Q5MsjhuZnml1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "722098e6"
      },
      "source": [
        "!pip install geopandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c264b0c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aab2cfaf"
      },
      "source": [
        "import geopandas as gpd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3123f46a"
      },
      "source": [
        "## Specify the file path\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8da6f914"
      },
      "source": [
        "geojson_file_path = '/content/drive/My Drive/earth_engine_exports/puntos_freatica_expanded.geojson'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70f3b3dd"
      },
      "source": [
        "## Read the geojson file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1922f877"
      },
      "source": [
        "gdf_puntos_muestreo = gpd.read_file(geojson_file_path)\n",
        "display(gdf_puntos_muestreo.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction and presence/absence points adjust\n",
        "Fusion de las variables en un solo dataframe."
      ],
      "metadata": {
        "id": "B1fIWPGOuflV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Feature Extraction (DEM, TWI, NDVI, NDWI) ---\n",
        "\n",
        "if gdf_puntos_muestreo is not None and roi_geometry is not None:\n",
        "    # Drop duplicate columns from gdf_puntos_muestreo before converting to EE FeatureCollection\n",
        "    gdf_puntos_muestreo = gdf_puntos_muestreo.loc[:,~gdf_puntos_muestreo.columns.duplicated()]\n",
        "    print(\"Dropped duplicate columns from gdf_puntos_muestreo.\")\n",
        "\n",
        "    # Load the SRTM image\n",
        "    srtm_image_id = 'USGS/SRTMGL1_003'\n",
        "    srtm_image = ee.Image(srtm_image_id)\n",
        "    print(\"SRTM image loaded.\")\n",
        "\n",
        "    # Filter SRTM image by ROI\n",
        "    filtered_srtm_image = srtm_image.clip(roi_geometry)\n",
        "    print(\"SRTM image filtered by ROI.\")\n",
        "\n",
        "    # Compute Slope and TWI\n",
        "    elevation_band = filtered_srtm_image.select('elevation')\n",
        "    slope_image = ee.Terrain.slope(elevation_band)\n",
        "    print(\"Slope calculated.\")\n",
        "\n",
        "    # Calculate TWI: ln(Flow Accumulation / tan(Slope))\n",
        "    # Ensure slope is in radians and avoid division by zero\n",
        "    # Corrected: Use ee.Image.tan() and ee.Image.log()\n",
        "    slope_radians = slope_image.divide(180).multiply(ee.Number(math.pi))\n",
        "    # Replace zero or near-zero slopes with a small positive number\n",
        "    min_slope_deg = 0.001\n",
        "    slope_corrected = slope_image.max(min_slope_deg)\n",
        "    slope_radians_corrected = slope_corrected.divide(180).multiply(ee.Number(math.pi))\n",
        "    tan_slope = slope_radians_corrected.tan()\n",
        "\n",
        "    # Replace zero or near-zero flow accumulation values\n",
        "    min_flow_acc = 1e-5 # Use a very small positive number\n",
        "    # Ensure flow_accumulation_ee_image exists and has the band 'b1'\n",
        "    if flow_accumulation_ee_image is not None and 'b1' in flow_accumulation_ee_image.bandNames().getInfo():\n",
        "        flow_accumulation_corrected = flow_accumulation_ee_image.select('b1').max(min_flow_acc) # Select the band if needed\n",
        "        # Calculate TWI\n",
        "        twi_image = flow_accumulation_corrected.divide(tan_slope).log().rename('TWI')\n",
        "        print(\"TWI calculated.\")\n",
        "\n",
        "        # Add DEM and TWI layers to map (optional visualization)\n",
        "        elevation_viz = {'min': 0, 'max': 4000, 'palette': ['006633', 'E5FFCC', '662A00', 'FF8040', '000000']}\n",
        "        # Dynamically calculate TWI viz min/max if possible, otherwise use defaults\n",
        "        try:\n",
        "            twi_min_max = twi_image.reduceRegion(reducer=ee.Reducer.minMax(), geometry=roi_geometry, scale=30, maxPixels=1e9)\n",
        "            twi_min = twi_min_max.get('TWI_min').getInfo() if twi_min_max.get('TWI_min') is not None else 0\n",
        "            twi_max = twi_min_max.get('TWI_max').getInfo() if twi_min_max.get('TWI_max') is not None else 10\n",
        "            twi_viz = {'min': twi_min, 'max': twi_max, 'palette': ['brown', 'yellow', 'green', 'cyan', 'blue']}\n",
        "            Map.addLayer(filtered_srtm_image.select('elevation'), elevation_viz, 'SRTM Elevation Model')\n",
        "            Map.addLayer(twi_image, twi_viz, 'Topographic Wetness Index (TWI)')\n",
        "            print(\"DEM and TWI layers added to map.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: Could not dynamically calculate TWI min/max or add layers: {e}\")\n",
        "             # Add layers with default visualization if dynamic calculation fails\n",
        "             twi_viz_default = {'min': 0, 'max': 10, 'palette': ['brown', 'yellow', 'green', 'cyan', 'blue']}\n",
        "             Map.addLayer(filtered_srtm_image.select('elevation'), elevation_viz, 'SRTM Elevation Model')\n",
        "             Map.addLayer(twi_image, twi_viz_default, 'Topographic Wetness Index (TWI) (Default Viz)')\n",
        "             print(\"DEM and TWI layers added to map with default visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "        # Convert the GeoDataFrame to an Earth Engine FeatureCollection for extraction\n",
        "        points_ee = geemap.gdf_to_ee(gdf_puntos_muestreo)\n",
        "        print(\"GeoDataFrame converted to Earth Engine FeatureCollection.\")\n",
        "\n",
        "        # Extract DEM and TWI values at sample points\n",
        "        # Ensure the band names exist in the images before selecting\n",
        "        dem_values_ee = filtered_srtm_image.select('elevation').reduceRegions(\n",
        "            collection=points_ee, reducer=ee.Reducer.first(), scale=30\n",
        "        )\n",
        "        twi_values_ee = twi_image.select('TWI').reduceRegions(\n",
        "            collection=points_ee, reducer=ee.Reducer.first(), scale=30\n",
        "        )\n",
        "        dem_df = geemap.ee_to_df(dem_values_ee)\n",
        "        twi_df = geemap.ee_to_df(twi_values_ee)\n",
        "        print(\"Extracted DEM and TWI values.\")\n",
        "        # Drop original 'Nelfle' column if it exists and merge DEM/TWI\n",
        "        if 'Nelfle' in gdf_puntos_muestreo.columns:\n",
        "            gdf_puntos_muestreo = gdf_puntos_muestreo.drop(columns=['Nelfle'])\n",
        "        # Ensure 'first' column exists before renaming\n",
        "        if 'first' in dem_df.columns:\n",
        "            dem_df = dem_df.rename(columns={'first': 'DEM'})\n",
        "        if 'first' in twi_df.columns:\n",
        "             twi_df = twi_df.rename(columns={'first': 'TWI'})\n",
        "\n",
        "        # Merge based on 'Site' or other common columns\n",
        "        # Check if 'Site' column exists in both dataframes before merging\n",
        "        if 'Site' in gdf_puntos_muestreo.columns and 'Site' in dem_df.columns and 'Site' in twi_df.columns:\n",
        "            merged_gdf = gdf_puntos_muestreo.merge(dem_df[['Site', 'DEM']], on='Site', how='left')\n",
        "            merged_gdf = merged_gdf.merge(twi_df[['Site', 'TWI']], on='Site', how='left')\n",
        "            gdf_puntos_muestreo = merged_gdf # Update the main GeoDataFrame\n",
        "            print(\"DEM and TWI merged into GeoDataFrame.\")\n",
        "        else:\n",
        "            print(\"Warning: 'Site' column not found in all necessary dataframes for DEM/TWI merge. Skipping merge.\")\n",
        "            # Handle cases where merge cannot be done\n",
        "            # If merge fails, gdf_puntos_muestreo will still have original columns + possibly Nelfle\n",
        "\n",
        "        # --- Generate multi-year median images (NDVI and NDWI) ---\n",
        "        # Function to calculate NDVI and NDWI (already defined above, but included again for clarity in this single block)\n",
        "        def add_indices(image):\n",
        "            ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "            ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
        "            return image.addBands([ndvi, ndwi])\n",
        "\n",
        "        # Function to get median by season (already defined above, but included again for clarity)\n",
        "        def get_season_median(year, months, season_name):\n",
        "            start = ee.Date.fromYMD(year, months[0], 1)\n",
        "            end = ee.Date.fromYMD(year, months[-1], 30)\n",
        "            collection = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "                          .filterDate(start, end)\n",
        "                          .filterBounds(roi_geometry)\n",
        "                          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n",
        "                          .map(add_indices)\n",
        "                          .select(['NDVI', 'NDWI']))\n",
        "            median = collection.median().clip(roi_geometry)\n",
        "            return median.set({'year': year, 'season': season_name})\n",
        "\n",
        "        # Generate multi-year median collections\n",
        "        dry_months = [2, 3, 4]\n",
        "        wet_months = [7, 8, 9]\n",
        "        dry_list = []\n",
        "        wet_list = []\n",
        "        for year in range(2019, 2026):\n",
        "            dry = get_season_median(year, dry_months, 'dry')\n",
        "            wet = get_season_median(year, wet_months, 'wet')\n",
        "            dry_list.append(dry)\n",
        "            wet_list.append(wet)\n",
        "        dry_collection = ee.ImageCollection(dry_list)\n",
        "        wet_collection = ee.ImageCollection(wet_list)\n",
        "        print(\"Multi-year NDVI and NDWI median collections created.\")\n",
        "\n",
        "        # Extract multi-year NDVI and NDWI values at sample points\n",
        "        all_ndvi_ndwi_data = []\n",
        "        for year in range(2019, 2026):\n",
        "            # Filter collections by year\n",
        "            dry_img_year_collection = dry_collection.filter(ee.Filter.eq('year', year))\n",
        "            wet_img_year_collection = wet_collection.filter(ee.Filter.eq('year', year))\n",
        "\n",
        "            # Check if collections are not empty for the year\n",
        "            if dry_img_year_collection.size().getInfo() > 0:\n",
        "                dry_img_year = dry_img_year_collection.first()\n",
        "                dry_extracted = dry_img_year.select(['NDVI', 'NDWI']).reduceRegions(\n",
        "                    collection=points_ee, reducer=ee.Reducer.first(), scale=10\n",
        "                )\n",
        "                dry_list = dry_extracted.getInfo()['features']\n",
        "                for feature in dry_list:\n",
        "                    properties = feature['properties']\n",
        "                    properties['year'] = year\n",
        "                    properties['season'] = 'dry'\n",
        "                    all_ndvi_ndwi_data.append(properties)\n",
        "            else:\n",
        "                print(f\"Warning: Dry season collection for year {year} is empty. Skipping extraction.\")\n",
        "\n",
        "            if wet_img_year_collection.size().getInfo() > 0:\n",
        "                wet_img_year = wet_img_year_collection.first()\n",
        "                wet_extracted = wet_img_year.select(['NDWI', 'NDVI']).reduceRegions( # Ensure correct bands are selected\n",
        "                    collection=points_ee, reducer=ee.Reducer.first(), scale=10\n",
        "                )\n",
        "                wet_list = wet_extracted.getInfo()['features']\n",
        "                for feature in wet_list:\n",
        "                    properties = feature['properties']\n",
        "                    properties['year'] = year\n",
        "                    properties['season'] = 'wet'\n",
        "                    all_ndvi_ndwi_data.append(properties)\n",
        "            else:\n",
        "                 print(f\"Warning: Wet season collection for year {year} is empty. Skipping extraction.\")\n",
        "\n",
        "\n",
        "        ndvi_ndwi_df = pd.DataFrame(all_ndvi_ndwi_data)\n",
        "        print(\"Multi-year NDVI and NDWI values extracted.\")\n",
        "\n",
        "        # Combine all extracted data\n",
        "        # Pivot ndvi_ndwi_df\n",
        "        # Ensure ndvi_ndwi_df is not empty and has expected columns\n",
        "        if not ndvi_ndwi_df.empty and 'Site' in ndvi_ndwi_df.columns and 'year' in ndvi_ndwi_df.columns and 'season' in ndvi_ndwi_df.columns and 'NDVI' in ndvi_ndwi_df.columns and 'NDWI' in ndvi_ndwi_df.columns:\n",
        "            ndvi_ndwi_pivot_df = ndvi_ndwi_df[['Site', 'year', 'season', 'NDVI', 'NDWI']].copy()\n",
        "            ndvi_ndwi_pivot_df['year_season'] = ndvi_ndwi_pivot_df['year'].astype(str) + '_' + ndvi_ndwi_pivot_df['season']\n",
        "            ndvi_ndwi_pivot_df = ndvi_ndwi_pivot_df.drop(columns=['year', 'season'])\n",
        "            # Check for duplicate index entries before setting index and unstacking\n",
        "            if not ndvi_ndwi_pivot_df.set_index(['Site', 'year_season']).index.is_unique:\n",
        "                 print(\"Warning: Duplicate 'Site', 'year_season' combinations found. Aggregating before pivoting.\")\n",
        "                 # Aggregate by taking the mean or first value for duplicates if necessary\n",
        "                 ndvi_ndwi_pivot_df = ndvi_ndwi_pivot_df.groupby(['Site', 'year_season']).mean().reset_index() # Example: take mean\n",
        "\n",
        "            ndvi_ndwi_pivot_df = ndvi_ndwi_pivot_df.set_index(['Site', 'year_season'])\n",
        "            pivoted_ndvi_ndwi = ndvi_ndwi_pivot_df.unstack()\n",
        "            pivoted_ndvi_ndwi.columns = ['_'.join(col).strip() for col in pivoted_ndvi_ndwi.columns.values]\n",
        "            pivoted_ndvi_ndwi = pivoted_ndvi_ndwi.reset_index()\n",
        "            # Merge with GeoDataFrame (which now has DEM and TWI)\n",
        "            # Ensure gdf_puntos_muestreo is not None before merging\n",
        "            if gdf_puntos_muestreo is not None and 'Site' in pivoted_ndvi_ndwi.columns and 'Site' in gdf_puntos_muestreo.columns:\n",
        "                 data_for_modeling = gdf_puntos_muestreo.merge(pivoted_ndvi_ndwi, on='Site', how='left')\n",
        "                 # Drop duplicate DEM and TWI columns if they exist and rename the kept ones\n",
        "                 # Assuming DEM_x and TWI_x are the desired columns after merge\n",
        "                 if 'DEM_x' in data_for_modeling.columns and 'DEM_y' in data_for_modeling.columns:\n",
        "                     data_for_modeling = data_for_modeling.drop(columns=['DEM_y'])\n",
        "                     data_for_modeling = data_for_modeling.rename(columns={'DEM_x': 'DEM'})\n",
        "                 elif 'DEM_y' in data_for_modeling.columns: # Handle case if only DEM_y exists after some merge issue\n",
        "                      data_for_modeling = data_for_modeling.rename(columns={'DEM_y': 'DEM'})\n",
        "\n",
        "                 if 'TWI_x' in data_for_modeling.columns and 'TWI_y' in data_for_modeling.columns:\n",
        "                     data_for_modeling = data_for_modeling.drop(columns=['TWI_y'])\n",
        "                     data_for_modeling = data_for_modeling.rename(columns={'TWI_x': 'TWI'})\n",
        "                 elif 'TWI_y' in data_for_modeling.columns: # Handle case if only TWI_y exists after some merge issue\n",
        "                      data_for_modeling = data_for_modeling.rename(columns={'TWI_y': 'TWI'})\n",
        "\n",
        "\n",
        "                 print(\"All data combined for modeling.\")\n",
        "                 display(data_for_modeling.head())\n",
        "            else:\n",
        "                 print(\"Warning: Cannot combine data for modeling. Check if gdf_puntos_muestreo or pivoted_ndvi_ndwi is valid or if 'Site' column exists.\")\n",
        "                 data_for_modeling = None # Set to None if merge fails\n",
        "        else:\n",
        "            print(\"Warning: Cannot pivot NDVI/NDWI data. Check if ndvi_ndwi_df is empty or missing columns.\")\n",
        "            data_for_modeling = gdf_puntos_muestreo # Use only DEM/TWI data if NDVI/NDWI extraction/pivoting fails\n",
        "\n",
        "    else:\n",
        "        print(\"Warning: TWI image could not be calculated. Skipping TWI extraction and combining.\")\n",
        "        # Proceed with data_for_modeling using only DEM and original columns if TWI failed\n",
        "        data_for_modeling = gdf_puntos_muestreo # Use gdf_puntos_muestreo which should have DEM\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Error: Cannot proceed with feature extraction. Ensure gdf_puntos_muestreo and roi_geometry are loaded correctly.\")\n",
        "    data_for_modeling = None # Set to None if data loading failed"
      ],
      "metadata": {
        "id": "gnxchNkcudSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30b5c534"
      },
      "source": [
        "## Handle missing values\n",
        "\n",
        "Address any missing data in the combined DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12f580a2"
      },
      "source": [
        "# Inspect missing values\n",
        "print(\"Missing values before handling:\")\n",
        "print(data_for_modeling.isnull().sum())\n",
        "\n",
        "# Identify columns with missing values\n",
        "missing_columns = data_for_modeling.columns[data_for_modeling.isnull().any()].tolist()\n",
        "print(f\"\\nColumns with missing values: {missing_columns}\")\n",
        "\n",
        "# Imputation strategy: Use the mean for numerical columns.\n",
        "# The missing values are likely in the multi-year NDVI and NDWI columns\n",
        "# due to cloud cover or lack of satellite data for specific years/seasons/locations.\n",
        "# Imputing with the mean of available data for that specific index (e.g., mean NDVI for 2022_dry)\n",
        "# is a reasonable starting point.\n",
        "\n",
        "# Apply imputation\n",
        "for col in missing_columns:\n",
        "    # Check if the column is numerical before attempting mean imputation\n",
        "    if pd.api.types.is_numeric_dtype(data_for_modeling[col]):\n",
        "        mean_value = data_for_modeling[col].mean()\n",
        "        data_for_modeling[col].fillna(mean_value, inplace=True)\n",
        "        print(f\"Imputed missing values in '{col}' with the mean ({mean_value:.4f}).\")\n",
        "    else:\n",
        "        print(f\"Column '{col}' is not numerical. Skipping mean imputation.\")\n",
        "\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(data_for_modeling.isnull().sum())\n",
        "\n",
        "# Display the first few rows to see the result\n",
        "display(data_for_modeling.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4455f6da"
      },
      "source": [
        "## Define features and target\n",
        "\n",
        "Separate the features (predictor variables) from the target variable (presence/absence of groundwater). Separate the target variable 'presencia' into y and the remaining feature columns into X. Include 'DEM' and 'TWI', and the multi-year NDVI and NDWI columns as features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4493def9"
      },
      "source": [
        "# Define the target variable\n",
        "y = data_for_modeling['presencia']\n",
        "\n",
        "# Define the features (predictor variables)\n",
        "# Exclude 'Site', 'Longitud', 'Latitud', 'PresFrea', 'presencia', and 'geometry'\n",
        "# Include 'DEM', 'TWI', and all the multi-year NDVI and NDWI columns, and 'block_id'\n",
        "feature_columns = [col for col in data_for_modeling.columns if col not in ['Site', 'Longitud', 'Latitud', 'PresFrea', 'presencia', 'geometry']]\n",
        "\n",
        "X = data_for_modeling[feature_columns]\n",
        "\n",
        "print(\"Features (X) shape:\", X.shape)\n",
        "print(\"Target (y) shape:\", y.shape)\n",
        "\n",
        "# Display the first few rows of the features and target\n",
        "print(\"\\nFeatures (X) head:\")\n",
        "display(X.head())\n",
        "\n",
        "print(\"\\nTarget (y) head:\")\n",
        "display(y.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eef8ae18"
      },
      "source": [
        "## Define Spatial Blocks using K-Means Clustering\n",
        "\n",
        "Apply K-Means clustering to the point coordinates to create spatial blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5712ef50"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the number of blocks you want to create.\n",
        "# A smaller number of blocks might increase the chance of having both classes in each block,\n",
        "# but reduces the number of cross-validation folds.\n",
        "n_blocks = 5 # You can adjust this number\n",
        "\n",
        "print(f\"Defining spatial blocks based on latitude quantiles with {n_blocks} blocks.\")\n",
        "\n",
        "# Create blocks based on latitude quantiles.\n",
        "# This divides the data points into blocks based on their latitude,\n",
        "# aiming for a roughly equal number of points in each block.\n",
        "data_for_modeling['block_id'] = pd.qcut(data_for_modeling['Latitud'], q=n_blocks, labels=False) + 1\n",
        "\n",
        "print(f\"Spatial blocks created based on {n_blocks} latitude quantiles.\")\n",
        "print(\"Added 'block_id' column to data_for_modeling.\")\n",
        "\n",
        "# Check the distribution of data points per block\n",
        "print(\"\\nDistribution of data points per block:\")\n",
        "print(data_for_modeling['block_id'].value_counts().sort_index())\n",
        "\n",
        "# Check the distribution of classes within each block\n",
        "print(\"\\nDistribution of classes within each block:\")\n",
        "print(data_for_modeling.groupby('block_id')['presencia'].value_counts().unstack(fill_value=0))\n",
        "\n",
        "# Display the first few rows with the new 'block_id'\n",
        "display(data_for_modeling.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cffc06be"
      },
      "source": [
        "## Split data\n",
        "\n",
        "Split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b63c65e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (e.g., 80% train, 20% test)\n",
        "# Using a fixed random_state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training features shape:\", X_train.shape)\n",
        "print(\"Testing features shape:\", X_test.shape)\n",
        "print(\"Training target shape:\", y_train.shape)\n",
        "print(\"Testing target shape:\", y_test.shape)\n",
        "\n",
        "# Check the distribution of the target variable in train and test sets\n",
        "print(\"\\nTraining target distribution:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nTesting target distribution:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature preparation for modeling"
      ],
      "metadata": {
        "id": "JOzSHQh-vAXQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd05e099"
      },
      "source": [
        "We will apply a Random Forest model with SMOTE for balancing and Block Cross-Validation with hyperparameter tuning using GridSearch on the combined DataFrame, after unifying the 'DEM_x' and 'DEM_y' columns into a single 'DEM' column, and the 'TWI_x' and 'TWI_y' columns into a single 'TWI' column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a858ea8f"
      },
      "source": [
        "## Define features and target\n",
        "\n",
        "Separate the features (predictor variables) from the target variable (presence/absence of groundwater). Separate the target variable 'presencia' into y and the remaining feature columns into X.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81166a9"
      },
      "source": [
        "## Address class imbalance with SMOTE\n",
        "\n",
        "Apply SMOTE to the training data to handle class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18fbb3e1"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to the training data only\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Shape of X_train before SMOTE:\", X_train.shape)\n",
        "print(\"Shape of X_train after SMOTE:\", X_train_res.shape)\n",
        "print(\"Shape of y_train before SMOTE:\", y_train.shape)\n",
        "print(\"Shape of y_train after SMOTE:\", y_train_res.shape)\n",
        "\n",
        "print(\"\\nDistribution of target variable before SMOTE:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(\"\\nDistribution of target variable after SMOTE:\")\n",
        "print(y_train_res.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ca693e"
      },
      "source": [
        "## Define the Random Forest model\n",
        "\n",
        "Initialize the Random Forest Classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1163f61"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "# We'll use default parameters for now, hyperparameter tuning will be done later with GridSearch\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "print(\"Random Forest model initialized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "205220b1"
      },
      "source": [
        "## Set up Hyperparameter Tuning with GridSearch and Block Cross-Validation\n",
        "\n",
        "Define the parameter grid for GridSearch and configure it with Block Cross-Validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e273394f"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold, GroupKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE # Import SMOTE\n",
        "from imblearn.pipeline import Pipeline # Import Pipeline\n",
        "import pandas as pd # Import pandas to access the DataFrame\n",
        "\n",
        "# Define the parameter grid to tune for the RandomForestClassifier\n",
        "# Note: Parameters for SMOTE can also be tuned if needed, by adding steps to the pipeline.\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [None, 10, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__criterion': ['gini', 'entropy']\n",
        "    # Example of tuning SMOTE parameters:\n",
        "    # 'smote__k_neighbors': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Create a pipeline that first applies SMOTE and then trains the RandomForestClassifier\n",
        "# The SMOTE step will only be applied to the training data within each CV fold.\n",
        "pipeline = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)), # Use SMOTE with a fixed random_state\n",
        "    ('classifier', RandomForestClassifier(random_state=42)) # Use RandomForestClassifier\n",
        "])\n",
        "\n",
        "# For Block Cross-Validation, we use GroupKFold with the 'block_id' column.\n",
        "# We need to provide the groups (block_id) to the split method of GroupKFold.\n",
        "\n",
        "# Ensure 'block_id' is in the training data (X_train).\n",
        "# Assuming 'block_id' was added to data_for_modeling before the train-test split\n",
        "# and is therefore included in X_train.\n",
        "if 'block_id' in X_train.columns:\n",
        "    # Get the block IDs for the original training data (X_train)\n",
        "    groups_for_cv = X_train['block_id']\n",
        "    # Drop 'block_id' from X_train as it's used for grouping, not as a feature for the model\n",
        "    X_train_no_block = X_train.drop(columns=['block_id'])\n",
        "\n",
        "    # Set up GroupKFold\n",
        "    # n_splits should be less than or equal to the number of unique blocks\n",
        "    n_splits_cv = min(5, groups_for_cv.nunique()) # Use up to 5 splits, not more than unique blocks\n",
        "    cv_strategy = GroupKFold(n_splits=n_splits_cv)\n",
        "    print(f\"GroupKFold Cross-Validation set up with {n_splits_cv} splits based on 'block_id'.\")\n",
        "\n",
        "    # Set up GridSearchCV\n",
        "    # We use the pipeline as the estimator and the original training data (X_train_no_block, y_train)\n",
        "    # The 'groups' parameter will be provided to the fit method.\n",
        "    grid_search = GridSearchCV(estimator=pipeline,\n",
        "                               param_grid=param_grid,\n",
        "                               cv=cv_strategy,\n",
        "                               scoring='f1',  # Using F1-score as the scoring metric\n",
        "                               n_jobs=-1,     # Use all available cores\n",
        "                               verbose=2)    # Increase verbosity to see progress\n",
        "\n",
        "    print(\"GridSearch with Pipeline (SMOTE + RandomForest) and GroupKFold Cross-Validation set up.\")\n",
        "    print(\"SMOTE will be applied within each CV fold's training subset.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'block_id' not found in X_train. Cannot proceed with GroupKFold.\")\n",
        "    grid_search = None # Cannot set up grid search without blocks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aef4781"
      },
      "source": [
        "## Train the model with GridSearch\n",
        "\n",
        "Train the Random Forest model using the training data with the configured GridSearch to find the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c354b0b3"
      },
      "source": [
        "# Train the model using GridSearchCV on the original training data\n",
        "print(\"Starting GridSearch training...\")\n",
        "# Provide the original training data (X_train_no_block, y_train) and the groups_for_cv\n",
        "# X_train_no_block is X_train with the 'block_id' column dropped.\n",
        "grid_search.fit(X_train_no_block, y_train, groups=groups_for_cv)\n",
        "\n",
        "\n",
        "print(\"\\nGridSearch training complete.\")\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "print(\"Best F1-score on training data:\", grid_search.best_score_)\n",
        "\n",
        "# Store the best model from the pipeline\n",
        "best_pipeline_model = grid_search.best_estimator_\n",
        "print(\"\\nBest Pipeline model (SMOTE + RandomForest) stored.\")\n",
        "\n",
        "# You can access the best RandomForest model from the pipeline like this:\n",
        "best_rf_model = best_pipeline_model.named_steps['classifier']\n",
        "print(\"Best RandomForest model extracted from the pipeline.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a36d51c0"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Evaluate the trained model using the test set with appropriate metrics (e.g., accuracy, precision, recall, F1-score, ROC AUC)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "417b5a12"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set using the best pipeline model\n",
        "# We need to drop the 'block_id' column from X_test before making predictions,\n",
        "# as the model was trained without it.\n",
        "if 'block_id' in X_test.columns:\n",
        "    X_test_no_block = X_test.drop(columns=['block_id'])\n",
        "else:\n",
        "    X_test_no_block = X_test # Use X_test directly if block_id is not present\n",
        "\n",
        "\n",
        "y_pred = best_pipeline_model.predict(X_test_no_block)\n",
        "y_prob = best_pipeline_model.predict_proba(X_test_no_block)[:, 1] # Get probabilities for the positive class\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Model Evaluation on the Test Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "\n",
        "# Interpret the confusion matrix:\n",
        "# [[True Negatives  False Positives]\n",
        "#  [False Negatives True Positives]]\n",
        "print(\"\\nConfusion Matrix Interpretation:\")\n",
        "print(f\"True Positives (correctly predicted presence): {conf_matrix[1, 1]}\")\n",
        "print(f\"True Negatives (correctly predicted absence): {conf_matrix[0, 0]}\")\n",
        "print(f\"False Positives (predicted presence, but actually absence): {conf_matrix[0, 1]}\")\n",
        "print(f\"False Negatives (predicted absence, but actually presence): {conf_matrix[1, 0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e0b658f"
      },
      "source": [
        "# Apply RF trained to original ROI\n",
        "Predict the presence of groundwater using a trained Random Forest model with SMOTE and Block Cross-Validation on the original area of interest defined by the Earth Engine asset 'users/facu_ruarte/Sur_SanJuan_export'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f5d3197"
      },
      "source": [
        "## Define prediction area geometry\n",
        "\n",
        "Load the geometry for the original prediction area from the Earth Engine asset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddf82c70"
      },
      "source": [
        "# Define the Earth Engine asset ID for the original prediction area\n",
        "original_area_asset_id = 'users/facu_ruarte/Sur_SanJuan_export'\n",
        "\n",
        "# Load the asset as an Earth Engine FeatureCollection\n",
        "try:\n",
        "    original_area_feature_collection = ee.FeatureCollection(original_area_asset_id)\n",
        "    print(f\"Successfully loaded original area asset: {original_area_asset_id}\")\n",
        "\n",
        "    # Extract the geometry from the FeatureCollection.\n",
        "    # Using .geometry().bounds() to get the bounding box geometry\n",
        "    prediction_area_geometry = original_area_feature_collection.geometry() # Using .geometry() to get the merged geometry\n",
        "    print(\"Extracted geometry for the original prediction area.\")\n",
        "\n",
        "except ee.EEException as e:\n",
        "    print(f\"Error loading Earth Engine asset: {e}\")\n",
        "    print(f\"Please ensure the asset ID '{original_area_asset_id}' is correct and the asset is publicly accessible or in your Earth Engine account.\")\n",
        "    prediction_area_geometry = None # Set geometry to None if loading fails\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    prediction_area_geometry = None # Set geometry to None if any other error occurs\n",
        "\n",
        "# Check if the geometry was successfully loaded\n",
        "if prediction_area_geometry is not None:\n",
        "    print(\"Prediction area geometry is loaded and ready for use.\")\n",
        "else:\n",
        "    print(\"Failed to load prediction area geometry.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dde3d4a9"
      },
      "source": [
        "## Extract features for prediction area\n",
        "\n",
        "### Subtask:\n",
        "Obtain the necessary predictor features (DEM, TWI, multi-year NDVI, NDWI) as raster layers for the entire prediction area by calculating or loading them from Earth Engine assets and stacking them into a single multi-band image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24eee48"
      },
      "source": [
        "## Extract features for prediction area\n",
        "\n",
        "Obtain the necessary predictor features (DEM, TWI, multi-year NDVI, NDWI) as raster layers for the entire prediction area.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56d03d46"
      },
      "source": [
        "# Define the Earth Engine asset ID for the original prediction area\n",
        "original_area_asset_id = 'users/facu_ruarte/Sur_SanJuan_Export'\n",
        "\n",
        "# Load the asset as an Earth Engine FeatureCollection\n",
        "try:\n",
        "    original_area_feature_collection = ee.FeatureCollection(original_area_asset_id)\n",
        "    print(f\"Successfully loaded original area asset: {original_area_asset_id}\")\n",
        "\n",
        "    # Extract the geometry from the FeatureCollection.\n",
        "    # Using .geometry() to get the merged geometry of all features in the collection\n",
        "    prediction_area_geometry = original_area_feature_collection.geometry()\n",
        "    print(\"Extracted geometry for the original prediction area.\")\n",
        "\n",
        "except ee.EEException as e:\n",
        "    print(f\"Error loading Earth Engine asset: {e}\")\n",
        "    print(f\"Please ensure the asset ID '{original_area_asset_id}' is correct and the asset is publicly accessible or in your Earth Engine account.\")\n",
        "    prediction_area_geometry = None # Set geometry to None if loading fails\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    prediction_area_geometry = None # Set geometry to None if any other error occurs\n",
        "\n",
        "# Check if the geometry was successfully loaded\n",
        "if prediction_area_geometry is not None:\n",
        "    print(\"Prediction area geometry is loaded and ready for use.\")\n",
        "else:\n",
        "    print(\"Failed to load prediction area geometry.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "321a7adc"
      },
      "source": [
        "Now we can proceed with obtaining the necessary predictor features (DEM, TWI, multi-year NDVI, NDWI) as raster layers for this prediction area.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53a2f2dd"
      },
      "source": [
        "import math # Ensure math is imported for calculations\n",
        "import ee # Ensure ee is imported\n",
        "\n",
        "# --- 1. Load and Filter DEM (SRTM) ---\n",
        "srtm_image_id = 'USGS/SRTMGL1_003'\n",
        "srtm_image = ee.Image(srtm_image_id)\n",
        "if prediction_area_geometry is not None:\n",
        "    # Select elevation and cast to Float32\n",
        "    dem_image = srtm_image.clip(prediction_area_geometry).select('elevation').toFloat()\n",
        "    print(\"DEM image loaded, clipped, and cast to Float32.\")\n",
        "else:\n",
        "    print(\"Warning: prediction_area_geometry is not available. Cannot clip DEM image. Casting to Float32.\")\n",
        "    dem_image = srtm_image.select('elevation').toFloat() # Use full image as fallback, cast to Float32\n",
        "\n",
        "\n",
        "# --- 2. Calculate Slope ---\n",
        "if dem_image is not None:\n",
        "    slope_image = ee.Terrain.slope(dem_image) # Slope calculation also works on Float\n",
        "    print(\"Slope image calculated.\")\n",
        "else:\n",
        "    print(\"Warning: DEM image not available. Cannot calculate slope.\")\n",
        "    slope_image = None # Set slope to None if DEM is not available\n",
        "\n",
        "# --- 3. Load and Clip Flow Accumulation ---\n",
        "flow_accumulation_asset_id = 'projects/identificacion-de-bosques/assets/acumulacion_flujo_expanded'\n",
        "try:\n",
        "    flow_accumulation_ee_image = ee.Image(flow_accumulation_asset_id)\n",
        "    if prediction_area_geometry is not None:\n",
        "        # Select band and cast to Float32\n",
        "        clipped_flow_accumulation = flow_accumulation_ee_image.clip(prediction_area_geometry).select('b1').toFloat()\n",
        "        print(\"Flow accumulation image loaded, clipped, and cast to Float32.\")\n",
        "    else:\n",
        "        print(\"Warning: prediction_area_geometry is not available. Cannot clip flow accumulation image. Casting to Float32.\")\n",
        "        clipped_flow_accumulation = flow_accumulation_ee_image.select('b1').toFloat() # Use full image as fallback, cast to Float32\n",
        "\n",
        "except ee.EEException as e:\n",
        "    print(f\"Error loading flow accumulation asset: {e}\")\n",
        "    clipped_flow_accumulation = None # Set to None if loading fails\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading flow accumulation: {e}\")\n",
        "    clipped_flow_accumulation = None\n",
        "\n",
        "\n",
        "# --- 4. Calculate TWI ---\n",
        "twi_image = None # Initialize TWI image as None\n",
        "if clipped_flow_accumulation is not None and slope_image is not None:\n",
        "    # Ensure slope is in radians and avoid division by zero\n",
        "    slope_radians = slope_image.divide(180).multiply(ee.Number(math.pi))\n",
        "    # Replace zero or near-zero slopes with a small positive number\n",
        "    min_slope_deg = 0.001\n",
        "    slope_corrected = slope_image.max(min_slope_deg)\n",
        "    slope_radians_corrected = slope_corrected.divide(180).multiply(ee.Number(math.pi))\n",
        "    tan_slope = slope_radians_corrected.tan()\n",
        "\n",
        "    # Replace zero or near-zero flow accumulation values\n",
        "    min_flow_acc = 1e-5 # Use a very small positive number\n",
        "    flow_accumulation_corrected = clipped_flow_accumulation.max(min_flow_acc) # Should already be Float32\n",
        "\n",
        "    # Calculate TWI: ln(Flow Accumulation / tan(Slope))\n",
        "    twi_image = flow_accumulation_corrected.divide(tan_slope).log().rename('TWI').toFloat() # Ensure TWI is Float32\n",
        "    print(\"TWI image calculated and cast to Float32.\")\n",
        "else:\n",
        "    print(\"Warning: Cannot calculate TWI. Flow accumulation or slope image is missing.\")\n",
        "\n",
        "\n",
        "# --- 5. Generate Multi-year Median NDVI and NDWI Collections ---\n",
        "# Function to calculate NDVI and NDWI (assuming add_indices is defined earlier)\n",
        "# def add_indices(image): ... # This function should also cast bands to Float if needed,\n",
        "# but normalizedDifference usually results in Float.\n",
        "\n",
        "def get_season_median_predict(year, months, season_name, geometry):\n",
        "    start = ee.Date.fromYMD(year, months[0], 1)\n",
        "    end = ee.Date.fromYMD(year, months[-1], 30)\n",
        "    collection = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "                  .filterDate(start, end)\n",
        "                  .filterBounds(geometry)\n",
        "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n",
        "                  .map(add_indices) # Assuming add_indices is defined and outputs Float bands\n",
        "                  .select(['NDVI', 'NDWI']))\n",
        "    median = collection.median()\n",
        "    if geometry is not None:\n",
        "        median = median.clip(geometry)\n",
        "    return median.set({'year': year, 'season': season_name}).toFloat() # Ensure median image is Float32\n",
        "\n",
        "\n",
        "dry_months = [2, 3, 4]\n",
        "wet_months = [7, 8, 9]\n",
        "prediction_years = range(2019, 2026) # Use the same years as in training data\n",
        "\n",
        "dry_median_images = []\n",
        "wet_median_images = []\n",
        "\n",
        "if prediction_area_geometry is not None:\n",
        "    for year in prediction_years:\n",
        "        dry_median = get_season_median_predict(year, dry_months, 'dry', prediction_area_geometry)\n",
        "        wet_median = get_season_median_predict(year, wet_months, 'wet', prediction_area_geometry)\n",
        "        dry_median_images.append(dry_median)\n",
        "        wet_median_images.append(wet_median)\n",
        "\n",
        "    print(\"Multi-year median NDVI and NDWI images generated and cast to Float32.\")\n",
        "else:\n",
        "    print(\"Warning: prediction_area_geometry not available. Cannot generate multi-year median images.\")\n",
        "\n",
        "\n",
        "# --- 6. Stack Features ---\n",
        "# Create a list of images to stack\n",
        "images_to_stack = []\n",
        "\n",
        "if dem_image is not None:\n",
        "    images_to_stack.append(dem_image) # Should already be Float32\n",
        "    print(\"Added DEM to stack list.\")\n",
        "if twi_image is not None:\n",
        "    images_to_stack.append(twi_image) # Should already be Float32\n",
        "    print(\"Added TWI to stack list.\")\n",
        "\n",
        "# Add multi-year NDVI and NDWI images to the list\n",
        "for img in dry_median_images:\n",
        "    # Select NDVI and NDWI bands and rename. Ensure they are Float32.\n",
        "    # get_season_median_predict should return Float32 images.\n",
        "    try:\n",
        "        year = img.get('year').getInfo()\n",
        "        bands = img.bandNames().getInfo()\n",
        "        if 'NDVI' in bands and 'NDWI' in bands:\n",
        "            dry_ndvi = img.select('NDVI').rename(f'NDVI_{year}_dry')\n",
        "            dry_ndwi = img.select('NDWI').rename(f'NDWI_{year}_dry')\n",
        "            images_to_stack.extend([dry_ndvi, dry_ndwi])\n",
        "            print(f\"Added NDVI_{year}_dry and NDWI_{year}_dry (Float32) to stack list.\")\n",
        "        else:\n",
        "             print(f\"Warning: NDVI or NDWI band not found in dry image for year {year}. Bands found: {bands}. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing dry median image: {e}. Skipping.\")\n",
        "\n",
        "\n",
        "for img in wet_median_images:\n",
        "    # Select NDVI and NDWI bands and rename. Ensure they are Float32.\n",
        "    # get_season_median_predict should return Float32 images.\n",
        "     try:\n",
        "        year = img.get('year').getInfo()\n",
        "        bands = img.bandNames().getInfo()\n",
        "        if 'NDVI' in bands and 'NDWI' in bands:\n",
        "            wet_ndvi = img.select('NDVI').rename(f'NDVI_{year}_wet')\n",
        "            wet_ndwi = img.select('NDWI').rename(f'NDWI_{year}_wet')\n",
        "            images_to_stack.extend([wet_ndvi, wet_ndwi])\n",
        "            print(f\"Added NDVI_{year}_wet and NDWI_{year}_wet (Float32) to stack list.\")\n",
        "        else:\n",
        "            print(f\"Warning: NDVI or NDWI band not found in wet image for year {year}. Bands found: {bands}. Skipping.\")\n",
        "     except Exception as e:\n",
        "        print(f\"Error processing wet median image: {e}. Skipping.\")\n",
        "\n",
        "\n",
        "# Check if there are images to stack\n",
        "if images_to_stack:\n",
        "    # Stack the images\n",
        "    stacked_features_image = ee.Image(images_to_stack)\n",
        "    print(\"\\nAll features stacked into a single image.\")\n",
        "\n",
        "    # Get the current band names of the stacked image\n",
        "    current_stacked_band_names = stacked_features_image.bandNames().getInfo()\n",
        "    print(\"\\nCurrent stacked image band names:\", current_stacked_band_names)\n",
        "\n",
        "    # Create the list of new band names, ensuring it matches the number of bands\n",
        "    # Based on the previous output, it seems the stack might be missing one band.\n",
        "    # Let's regenerate the expected band names based on the successfull additions to the list.\n",
        "    # Assuming X_train_no_block represents the training features used by the model\n",
        "    if 'X_train_no_block' in locals() and X_train_no_block is not None:\n",
        "        expected_training_feature_names = X_train_no_block.columns.tolist()\n",
        "    elif 'X' in locals() and X is not None and 'block_id' in X.columns:\n",
        "        expected_training_feature_names = [col for col in X.columns if col != 'block_id']\n",
        "    elif 'X' in locals() and X is not None:\n",
        "        expected_training_feature_names = X.columns.tolist()\n",
        "    else:\n",
        "        print(\"Warning: Could not reliably determine training feature names. Proceeding with caution.\")\n",
        "        # Fallback: manually construct expected band names based on the stacking process\n",
        "        expected_training_feature_names = ['DEM', 'TWI'] + [f'NDVI_{year}_dry' for year in prediction_years] + [f'NDWI_{year}_dry' for year in prediction_years] + [f'NDVI_{year}_wet' for year in prediction_years] + [f'NDWI_{year}_wet' for year in prediction_years]\n",
        "        # Need to filter this list based on which images were actually added to images_to_stack\n",
        "        # This is getting complex. Let's just use the current stacked band names and rename 'elevation' if present.\n",
        "        print(\"Using current stacked band names for renaming logic.\")\n",
        "\n",
        "\n",
        "    # Create a list of new band names based on the current stacked band names\n",
        "    new_band_names = []\n",
        "    for band_name in current_stacked_band_names:\n",
        "         if band_name == 'elevation':\n",
        "             new_band_names.append('DEM')\n",
        "         else:\n",
        "             new_band_names.append(band_name) # Keep other names as they are assumed to be correct\n",
        "\n",
        "\n",
        "    print(\"\\nGenerated new band names list for renaming:\", new_band_names)\n",
        "    print(\"Number of new band names:\", len(new_band_names))\n",
        "    print(\"Number of bands in stacked image:\", len(current_stacked_band_names))\n",
        "\n",
        "\n",
        "    # Rename the bands only if the number of names matches the number of bands\n",
        "    if len(new_band_names) == len(current_stacked_band_names):\n",
        "        try:\n",
        "            stacked_features_image = stacked_features_image.rename(new_band_names)\n",
        "            print(\"\\nSuccessfully renamed bands in the stacked image.\")\n",
        "\n",
        "            # Final verification of band names\n",
        "            stacked_band_names_final = stacked_features_image.bandNames().getInfo()\n",
        "            print(\"\\nFinal stacked image band names:\", stacked_band_names_final)\n",
        "            # Re-evaluate expected_training_feature_names if needed for comparison\n",
        "            if 'X_train_no_block' in locals() and X_train_no_block is not None:\n",
        "                 expected_training_feature_names_final = X_train_no_block.columns.tolist()\n",
        "            else:\n",
        "                 # Fallback or use the previous expected list if available\n",
        "                 expected_training_feature_names_final = expected_training_feature_names # Use the list generated earlier if possible\n",
        "\n",
        "            print(\"Training feature names (excluding block_id):\", expected_training_feature_names_final)\n",
        "\n",
        "            missing_bands_in_stack = [name for name in expected_training_feature_names_final if name not in stacked_band_names_final]\n",
        "            extra_bands_in_stack = [name for name in stacked_band_names_final if name not in expected_training_feature_names_final]\n",
        "\n",
        "            if not missing_bands_in_stack:\n",
        "                print(\"\\nAll training features have corresponding bands in the stacked image after final rename.\")\n",
        "            else:\n",
        "                print(f\"\\nWarning: The following training features are missing in the stacked image after final rename: {missing_bands_in_stack}\")\n",
        "\n",
        "            if not extra_bands_in_stack:\n",
        "                 print(\"No extra bands in the stacked image compared to training features after final rename.\")\n",
        "            else:\n",
        "                 print(f\"Warning: The following extra bands are present in the stacked image after final rename: {extra_bands_in_stack}\")\n",
        "\n",
        "\n",
        "        except ee.EEException as e:\n",
        "            print(f\"Error during final renaming: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during final renaming: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nError: Number of new band names does not match the number of bands in the stacked image. Renaming skipped.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nError: No images available to stack. Cannot create stacked features image.\")\n",
        "    stacked_features_image = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb9ZpBj7DrK_"
      },
      "source": [
        "import math # Ensure math is imported for calculations\n",
        "import ee # Ensure ee is imported\n",
        "\n",
        "# Print the band names of the individual images before stacking to diagnose the issue\n",
        "print(\"Band names of individual images before stacking:\")\n",
        "\n",
        "if dem_image is not None:\n",
        "    print(\"DEM image bands:\", dem_image.bandNames().getInfo())\n",
        "else:\n",
        "    print(\"DEM image is None.\")\n",
        "\n",
        "if twi_image is not None:\n",
        "    print(\"TWI image bands:\", twi_image.bandNames().getInfo())\n",
        "else:\n",
        "    print(\"TWI image is None.\")\n",
        "\n",
        "for i, img in enumerate(dry_median_images):\n",
        "    try:\n",
        "        print(f\"Dry median image {i+1} (Year {img.get('year').getInfo()}):\", img.bandNames().getInfo())\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting bands for dry median image {i+1}: {e}\")\n",
        "\n",
        "for i, img in enumerate(wet_median_images):\n",
        "    try:\n",
        "        print(f\"Wet median image {i+1} (Year {img.get('year').getInfo()}):\", img.bandNames().getInfo())\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting bands for wet median image {i+1}: {e}\")\n",
        "\n",
        "# Re-stack the images, carefully checking the process\n",
        "images_to_stack = []\n",
        "\n",
        "if dem_image is not None:\n",
        "    images_to_stack.append(dem_image) # Should already be Float32 from previous step\n",
        "    print(\"\\nAdded DEM to stack list.\")\n",
        "if twi_image is not None:\n",
        "    images_to_stack.append(twi_image) # Should already be Float32 from previous step\n",
        "    print(\"Added TWI to stack list.\")\n",
        "\n",
        "# Add multi-year NDVI and NDWI images to the list, ensuring bands exist and are Float32\n",
        "for img in dry_median_images:\n",
        "    try:\n",
        "        year = img.get('year').getInfo()\n",
        "        bands = img.bandNames().getInfo()\n",
        "        if 'NDVI' in bands and 'NDWI' in bands:\n",
        "            # Select bands and rename. Ensure they are Float32 (should be from get_season_median_predict)\n",
        "            dry_ndvi = img.select('NDVI').rename(f'NDVI_{year}_dry')\n",
        "            dry_ndwi = img.select('NDWI').rename(f'NDWI_{year}_dry')\n",
        "            images_to_stack.extend([dry_ndvi, dry_ndwi])\n",
        "            print(f\"Added NDVI_{year}_dry and NDWI_{year}_dry (Float32) to stack list.\")\n",
        "        else:\n",
        "             print(f\"Warning: NDVI or NDWI band not found in dry image for year {year}. Bands found: {bands}. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing dry median image: {e}. Skipping.\")\n",
        "\n",
        "\n",
        "for img in wet_median_images:\n",
        "    try:\n",
        "        year = img.get('year').getInfo()\n",
        "        bands = img.bandNames().getInfo()\n",
        "        if 'NDVI' in bands and 'NDWI' in bands:\n",
        "            # Select bands and rename. Ensure they are Float32 (should be from get_season_median_predict)\n",
        "            wet_ndvi = img.select('NDVI').rename(f'NDVI_{year}_wet')\n",
        "            wet_ndwi = img.select('NDWI').rename(f'NDWI_{year}_wet')\n",
        "            images_to_stack.extend([wet_ndvi, wet_ndwi])\n",
        "            print(f\"Added NDVI_{year}_wet and NDWI_{year}_wet (Float32) to stack list.\")\n",
        "        else:\n",
        "            print(f\"Warning: NDVI or NDWI band not found in wet image for year {year}. Bands found: {bands}. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing wet median image: {e}. Skipping.\")\n",
        "\n",
        "\n",
        "# Check if there are images to stack\n",
        "if images_to_stack:\n",
        "    # Stack the images\n",
        "    stacked_features_image = ee.Image(images_to_stack)\n",
        "    print(\"\\nAll features stacked into a single image.\")\n",
        "\n",
        "    # Get the current band names of the stacked image\n",
        "    current_stacked_band_names = stacked_features_image.bandNames().getInfo()\n",
        "    print(\"\\nCurrent stacked image band names:\", current_stacked_band_names)\n",
        "\n",
        "    # Create the list of new band names based on the current stacked band names\n",
        "    new_band_names = []\n",
        "    for band_name in current_stacked_band_names:\n",
        "         if band_name == 'elevation':\n",
        "             new_band_names.append('DEM')\n",
        "         else:\n",
        "             new_band_names.append(band_name) # Keep other names as they are assumed to be correct\n",
        "\n",
        "\n",
        "    print(\"\\nGenerated new band names list for renaming:\", new_band_names)\n",
        "    print(\"Number of new band names:\", len(new_band_names))\n",
        "    print(\"Number of bands in stacked image:\", len(current_stacked_band_names))\n",
        "\n",
        "\n",
        "    # Rename the bands only if the number of names matches the number of bands\n",
        "    if len(new_band_names) == len(current_stacked_band_names):\n",
        "        try:\n",
        "            stacked_features_image = stacked_features_image.rename(new_band_names)\n",
        "            print(\"\\nSuccessfully renamed bands in the stacked image.\")\n",
        "\n",
        "            # Final verification of band names\n",
        "            stacked_band_names_final = stacked_features_image.bandNames().getInfo()\n",
        "            print(\"\\nFinal stacked image band names:\", stacked_band_names_final)\n",
        "            # Re-evaluate expected_training_feature_names if needed for comparison\n",
        "            if 'X_train_no_block' in locals() and X_train_no_block is not None:\n",
        "                 expected_training_feature_names_final = X_train_no_block.columns.tolist()\n",
        "            elif 'X' in locals() and X is not None and 'block_id' in X.columns:\n",
        "                expected_training_feature_names_final = [col for col in X.columns if col != 'block_id']\n",
        "            elif 'X' in locals() and X is not None:\n",
        "                 expected_training_feature_names_final = X.columns.tolist()\n",
        "            else:\n",
        "                 print(\"Warning: Could not reliably determine training feature names. Cannot fully verify band names.\")\n",
        "                 expected_training_feature_names_final = [] # Cannot verify without training features\n",
        "\n",
        "\n",
        "            print(\"Training feature names (excluding block_id):\", expected_training_feature_names_final)\n",
        "\n",
        "            missing_bands_in_stack = [name for name in expected_training_feature_names_final if name not in stacked_band_names_final]\n",
        "            extra_bands_in_stack = [name for name in stacked_band_names_final if name not in expected_training_feature_names_final]\n",
        "\n",
        "            if expected_training_feature_names_final: # Only print if training feature names could be determined\n",
        "                if not missing_bands_in_stack:\n",
        "                    print(\"\\nAll training features have corresponding bands in the stacked image after final rename.\")\n",
        "                else:\n",
        "                    print(f\"\\nWarning: The following training features are missing in the stacked image after final rename: {missing_bands_in_stack}\")\n",
        "\n",
        "                if not extra_bands_in_stack:\n",
        "                     print(\"No extra bands in the stacked image compared to training features after final rename.\")\n",
        "                else:\n",
        "                     print(f\"Warning: The following extra bands are present in the stacked image after final rename: {extra_bands_in_stack}\")\n",
        "            else:\n",
        "                print(\"\\nSkipping final band name verification as training feature names could not be determined.\")\n",
        "\n",
        "\n",
        "        except ee.EEException as e:\n",
        "            print(f\"Error during final renaming: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during final renaming: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nError: Number of new band names does not match the number of bands in the stacked image. Renaming skipped.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nError: No images available to stack. Cannot create stacked features image.\")\n",
        "    stacked_features_image = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb105710"
      },
      "source": [
        "## Prepare feature data for prediction\n",
        "\n",
        "Convert the feature image into a format suitable for prediction with the trained model (e.g., an array or a flattened representation).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3d861c7"
      },
      "source": [
        "import numpy as np\n",
        "import geemap # Ensure geemap is imported\n",
        "import ee # Ensure ee is imported\n",
        "\n",
        "# Convert the stacked Earth Engine image to a numpy array\n",
        "# Use the prediction_area_geometry for the region\n",
        "# Use a suitable scale (e.g., 30 meters, matching the SRTM resolution)\n",
        "try:\n",
        "    # Get the scale from one of the images if available, or use a default\n",
        "    if stacked_features_image is not None:\n",
        "        # Attempt to get nominal scale, default to 30 if not available or problematic\n",
        "        scale = stacked_features_image.select(0).projection().nominalScale().getInfo()\n",
        "        if scale is None or scale <= 0:\n",
        "            scale = 30 # Fallback to 30 if nominalScale is not valid\n",
        "        print(f\"Using prediction scale: {scale} meters.\")\n",
        "    else:\n",
        "        scale = 30 # Default scale if stacked_features_image is None\n",
        "        print(f\"Warning: stacked_features_image is None. Using default scale: {scale} meters.\")\n",
        "\n",
        "    # Ensure prediction_area_geometry is not None before proceeding\n",
        "    if prediction_area_geometry is not None:\n",
        "        # Get the GeoJSON representation of the prediction area geometry\n",
        "        prediction_area_geojson = prediction_area_geometry.getInfo()\n",
        "        print(\"Obtained GeoJSON representation of the prediction area geometry.\")\n",
        "\n",
        "        # Pass the stacked image as the EE object and the GeoJSON as the region\n",
        "        features_array = geemap.ee_to_numpy(\n",
        "            ee_object=stacked_features_image, # Pass the stacked image here\n",
        "            region=prediction_area_geojson, # Use the GeoJSON as the region\n",
        "            scale=scale,\n",
        "            # Specify the data type to ensure compatibility with the model\n",
        "            dtype=np.float32 # Using float32 for efficiency and compatibility\n",
        "        )\n",
        "        print(\"\\nEarth Engine image converted to numpy array.\")\n",
        "        print(\"Original numpy array shape:\", features_array.shape)\n",
        "\n",
        "        # Reshape the numpy array for prediction\n",
        "        # The current shape is (rows, columns, features)\n",
        "        # We need to reshape it to (number of pixels, number of features)\n",
        "        # Number of pixels = rows * columns\n",
        "        # Number of features = the last dimension of the array\n",
        "        if features_array is not None and features_array.ndim == 3:\n",
        "            rows, cols, n_features = features_array.shape\n",
        "            features_array_reshaped = features_array.reshape(rows * cols, n_features)\n",
        "            print(\"Numpy array reshaped for prediction.\")\n",
        "            print(\"Reshaped numpy array shape:\", features_array_reshaped.shape)\n",
        "        elif features_array is not None and features_array.ndim == 2:\n",
        "             # Handle case where array might be 2D (e.g., single band image)\n",
        "             rows, cols = features_array.shape\n",
        "             n_features = 1\n",
        "             features_array_reshaped = features_array.reshape(rows * cols, n_features)\n",
        "             print(\"Numpy array reshaped (2D case) for prediction.\")\n",
        "             print(\"Reshaped numpy array shape:\", features_array_reshaped.shape)\n",
        "        else:\n",
        "            print(\"Error: Numpy array does not have expected dimensions (3 or 2). Cannot reshape.\")\n",
        "            features_array_reshaped = None # Set to None if reshaping fails\n",
        "\n",
        "\n",
        "        # Handle potential nodata values (often represented by NaNs)\n",
        "        # Replace NaN values with a default value, e.g., the mean of the valid data\n",
        "        if features_array_reshaped is not None:\n",
        "             if np.isnan(features_array_reshaped).any():\n",
        "                 print(\"\\nHandling NaN values in the reshaped array...\")\n",
        "                 # Compute mean only on non-NaN values\n",
        "                 mean_per_feature = np.nanmean(features_array_reshaped, axis=0)\n",
        "                 # Replace NaNs with the calculated mean for each feature column\n",
        "                 # Use np.nan_to_num for a robust replacement\n",
        "                 features_array_reshaped = np.nan_to_num(features_array_reshaped, nan=mean_per_feature)\n",
        "                 print(\"NaN values replaced with column means.\")\n",
        "             else:\n",
        "                 print(\"\\nNo NaN values found in the reshaped array.\")\n",
        "\n",
        "        # Verify the data type\n",
        "        if features_array_reshaped is not None:\n",
        "            print(\"Data type of reshaped array:\", features_array_reshaped.dtype)\n",
        "            if features_array_reshaped.dtype != np.float32:\n",
        "                 print(\"Warning: Reshaped array data type is not float32.\") # Optional warning\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Error: prediction_area_geometry is None. Cannot convert Earth Engine image to numpy array.\")\n",
        "        features_array = None\n",
        "        features_array_reshaped = None\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Earth Engine to numpy conversion or reshaping: {e}\")\n",
        "    features_array = None\n",
        "    features_array_reshaped = None\n",
        "\n",
        "# At this point, features_array_reshaped contains the pixel data in the correct shape for prediction\n",
        "# It's a numpy array of shape (number of pixels, number of features)\n",
        "# Make sure features_array_reshaped is not None before proceeding to the prediction step."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2672a5b5"
      },
      "source": [
        "## Export stacked features image\n",
        "\n",
        "Export the `stacked_features_image` from Earth Engine to a GeoTIFF file in your Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdMg2wsLFUSB"
      },
      "source": [
        "# Define the parameters for the export task\n",
        "task_description = 'Groundwater_Prediction_Features_Export'\n",
        "export_folder = 'earth_engine_exports'\n",
        "export_file_name = 'predicted_features'\n",
        "export_scale = 30  # Use a scale that matches your analysis (e.g., 30 meters for SRTM)\n",
        "\n",
        "# Ensure stacked_features_image is not None and prediction_area_geometry is not None\n",
        "if stacked_features_image is not None and prediction_area_geometry is not None:\n",
        "    # Create the export task\n",
        "    export_task = ee.batch.Export.image.toDrive(\n",
        "        image=stacked_features_image,\n",
        "        description=task_description,\n",
        "        folder=export_folder,\n",
        "        fileNamePrefix=export_file_name,\n",
        "        scale=export_scale,\n",
        "        region=prediction_area_geometry, # Pass the Earth Engine Geometry object directly\n",
        "        fileFormat='GeoTIFF',\n",
        "        crs='EPSG:4326' # Specify a coordinate reference system\n",
        "    )\n",
        "\n",
        "    # Start the export task\n",
        "    export_task.start()\n",
        "\n",
        "    print(f\"Earth Engine export task '{task_description}' started.\")\n",
        "    print(f\"Check the Earth Engine Tasks tab to monitor progress.\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: stacked_features_image or prediction_area_geometry is not available. Cannot start export task.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c64fd682"
      },
      "source": [
        "## Make Predictions\n",
        "Use the trained model (`best_pipeline_model`) to predict the presence/absence or probability for each pixel in the prediction area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0102f98"
      },
      "source": [
        "# Ensure the trained model and the reshaped features array are available\n",
        "if 'best_pipeline_model' in locals() and best_pipeline_model is not None and \\\n",
        "   'features_array_reshaped' in locals() and features_array_reshaped is not None:\n",
        "\n",
        "    print(\"Making predictions using the trained model...\")\n",
        "\n",
        "    # Make predictions (either classes or probabilities)\n",
        "    # To get predicted classes (0 or 1):\n",
        "    # predictions = best_pipeline_model.predict(features_array_reshaped)\n",
        "\n",
        "    # To get predicted probabilities (useful for visualization or further analysis):\n",
        "    # The output is an array of shape (n_samples, n_classes).\n",
        "    # We want the probability of the positive class (class 1).\n",
        "    prediction_probabilities = best_pipeline_model.predict_proba(features_array_reshaped)[:, 1]\n",
        "\n",
        "    print(\"Predictions made successfully.\")\n",
        "    print(\"Prediction probabilities shape:\", prediction_probabilities.shape)\n",
        "\n",
        "    # Store the prediction results (probabilities in this case)\n",
        "    prediction_results = prediction_probabilities\n",
        "\n",
        "else:\n",
        "    print(\"Error: Trained model (best_pipeline_model) or reshaped features (features_array_reshaped) not available. Cannot make predictions.\")\n",
        "    prediction_results = None # Set to None if prediction fails"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}